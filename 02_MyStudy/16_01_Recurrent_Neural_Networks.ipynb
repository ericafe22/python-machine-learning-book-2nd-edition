{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  I went and saw this movie last night after bei...          1\n",
      "1  Actor turned director Bill Paxton follows up h...          1\n",
      "2  As a recreational golfer with some knowledge o...          1\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストを一連の整数に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 単語に分割して各単語の出現回数をカウント\n",
    "# punctuationが入っていれば前後の文字も1つの単語としてカウント\n",
    "# punctuation :: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~.\n",
    "counts = Counter()\n",
    "\n",
    "for i,review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ''+c+'' for c in review]).lower()\n",
    "    df.loc[i, 'review'] = text\n",
    "    counts.update(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 1, 'b': 1, 'c': 1})\n",
      "Counter({'a': 2, 'b': 2, 'c': 2})\n"
     ]
    }
   ],
   "source": [
    "# カウンタークラスの検証\n",
    "c = Counter()\n",
    "c.update(['a','b','c'])\n",
    "print(c)\n",
    "c.update(['a','b','c'])\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'a', 'and', 'of', 'to']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# マッピングを作成\n",
    "# 一意な単語をそれぞれ整数にマッピング\n",
    "\n",
    "# カウンタを多い順にソート\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "\n",
    "# 出現数が多いものから順番に1～インデックスをはる\n",
    "# 辞書型\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)} # インデックスの取得を1から\n",
    "print(word_to_int[\"and\"])\n",
    "\n",
    "# REVIEWの文章に単語の出現順位をマップしていく\n",
    "mapped_reviews = []\n",
    "\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 長さを整える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNNは入力が同じ長さでないとだめ\n",
    "# sequence_length以下の場合 :: 左側を0パディング\n",
    "# sequence_length以上の場合 :: 左の数字を省略\n",
    "sequence_length = 200\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:] # 後ろから〇〇個の要素を取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを訓練用とテスト用に分ける\n",
    "X_train = sequences[:25000, :]\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ミニバッチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチ用の関数を定義\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    # 整数の商を取得\n",
    "    n_batches = len(x)//batch_size\n",
    "    \n",
    "    # 対象の数字までをスライス\n",
    "    x= x[:n_batches*batch_size]\n",
    "    \n",
    "    if y is not None:\n",
    "        y = y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        else:\n",
    "            yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 埋め込み(一意な単語をベクトルで表現)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-001119dcc43e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 埋め込み(一意な単語をベクトルで表現)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m embedding = tf.Variable(\n\u001b[1;32m----> 3\u001b[1;33m     tf.random_uniform(shape=(n_words, embedding_size),minval=1,maxval=1))\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# 一意な単語に対応するベクトルを特定\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0membed_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_words' is not defined"
     ]
    }
   ],
   "source": [
    "# 埋め込み(一意な単語をベクトルで表現)\n",
    "embedding = tf.Variable(\n",
    "    tf.random_uniform(shape=(n_words, embedding_size),minval=1,maxval=1))\n",
    "# 一意な単語に対応するベクトルを特定\n",
    "embed_x = tf.nn.embedding_lookup(embedding, tf_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNモデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 構成は以下 ###\n",
    "# コンストラクタ\n",
    "# buildメソッド\n",
    "# trainメソッド\n",
    "# predictメソッド\n",
    "\n",
    "class SentimentRNN(object):\n",
    "    def __init__(self\n",
    "                 , n_words\n",
    "                 , seq_len=200\n",
    "                 ,lstm_size=256\n",
    "                 , num_layers=1\n",
    "                 , batch_size=64\n",
    "                 ,learning_rate=0.0001\n",
    "                 , embed_size=200):\n",
    "            \n",
    "        self.n_words = n_words              # 一意な単語の数\n",
    "        self.seq_len = seq_len              # sequenceの長さ\n",
    "        self.lstm_size = lstm_size          # 隠れユニットの個数\n",
    "        self.num_layers = num_layers        # レイヤー数\n",
    "        self.batch_size = batch_size        # バッチサイズ\n",
    "        self.learning_rate = learning_rate  # 学習率\n",
    "        self.embed_size = embed_size        # 一意な単語を表現するためのベクトルの箱数\n",
    "\n",
    "        # 計算グラフを作成\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    def build(self):\n",
    "        # 各プレースホルダーを定義\n",
    "        # 入力データ\n",
    "        tf_x = tf.placeholder(\n",
    "            tf.int32\n",
    "            ,shape=(self.batch_size, self.seq_len)\n",
    "            ,name='tf_x')\n",
    "        # 入力ラベル\n",
    "        tf_y = tf.placeholder(\n",
    "            tf.float32\n",
    "            ,shape=(self.batch_size)\n",
    "            ,name='tf_y')\n",
    "        # ドロップアウトのキープ率\n",
    "        tf_keepprob = tf.placeholder(\n",
    "            tf.float32\n",
    "            ,name='tf_keepprob')\n",
    "        \n",
    "        # 埋め込み層を作成\n",
    "        embedding = tf.Variable(\n",
    "                    tf.random_uniform(\n",
    "                        (self.n_words, self.embed_size),\n",
    "                        minval=-1, maxval=1),\n",
    "                    name='embedding')\n",
    "        embed_x = tf.nn.embedding_lookup(\n",
    "                    embedding, tf_x, \n",
    "                    name='embeded_x')\n",
    "        \n",
    "        # LSTMセル(長短期記憶)を定義し、積み上げる\n",
    "        # tf.contrib.rnn.MultiRNNCell\n",
    "        cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [tf.nn.rnn_cell.DropoutWrapper(\n",
    "                   tf.nn.rnn_cell.BasicLSTMCell(self.lstm_size),\n",
    "                   output_keep_prob=tf_keepprob)\n",
    "                 for i in range(self.num_layers)])\n",
    "        \n",
    "        # LSTMの初期状態を定義(全て0を設定)\n",
    "        self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
    "        print('  << initial state >> ', self.initial_state)\n",
    "        \n",
    "        # LSTMのアウトプットと最終状態\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "            cells\n",
    "            ,embed_x\n",
    "            ,initial_state=self.initial_state)\n",
    "        \n",
    "        print('\\n  << lstm_output   >> ', lstm_outputs)      # バッチサイズ,最大時間,アウトプットサイズ\n",
    "        print('\\n  << final state   >> ', self.final_state)\n",
    "        \n",
    "        # RNNの出力後に全結合層を適用\n",
    "        # tf.layers.dense :: 全結合レイヤーを構築。引数としてニューロンの数と活性化関数をとる。\n",
    "        logits = tf.layers.dense(\n",
    "         inputs=lstm_outputs[:, -1],\n",
    "         units=1, activation=None,\n",
    "         name='logits')\n",
    "        \n",
    "        # tf.squeeze :: sizeが1の次元を削除し次元数を減らすAPI\n",
    "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "        print ('\\n  << logits        >> ', logits)\n",
    "        \n",
    "        # シグモイド関数を適用する\n",
    "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "        predictions = {\n",
    "            'probabilities': y_proba,\n",
    "            'labels' : tf.cast(tf.round(y_proba), tf.int32,\n",
    "                 name='labels')\n",
    "        }\n",
    "        print('\\n  << predictions   >> ', predictions)\n",
    "        \n",
    "        # コスト関数を定義する\n",
    "        # tf.reduce_mean :: 与えたリストに入っている数値の平均値を求める関数\n",
    "        cost = tf.reduce_mean(\n",
    "         tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "         labels=tf_y, logits=logits),\n",
    "         name='cost')\n",
    "        \n",
    "        # オプティマイザを定義する\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "\n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state) # セルの状態を初期状態に更新\n",
    "                \n",
    "                for batch_x, batch_y in create_batch_generator(\n",
    "                            X_train, y_train, self.batch_size):\n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0': 0.5,\n",
    "                            self.initial_state : state}\n",
    "                    loss, _, state = sess.run(\n",
    "                            ['cost:0', 'train_op', \n",
    "                             self.final_state],      # セルの状態を最終状態に更新\n",
    "                            feed_dict=feed)\n",
    "\n",
    "                    if iteration % 20 == 0:\n",
    "                        print(\"Epoch: %d/%d Iteration: %d \"\n",
    "                              \"| Train loss: %.5f\" % (\n",
    "                               epoch + 1, num_epochs,\n",
    "                               iteration, loss))\n",
    "\n",
    "                    iteration +=1 \n",
    "                    \n",
    "                if (epoch+1)%10 == 0:\n",
    "                    self.saver.save(sess,\n",
    "                        \"model/sentiment-%d.ckpt\" % epoch)\n",
    "                    \n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds=[]\n",
    "        with tf.Session(graph = self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess, tf.train.latest_checkpoint('model/'))\n",
    "            test_state = sess.run(self.initial_state)\n",
    "            for ii, batch_x in enumerate(\n",
    "                create_batch_generator(\n",
    "                    X_data, None, batch_size=self.batch_size), 1):\n",
    "                feed = {'tf_x:0' : batch_x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state : test_state}\n",
    "                if return_proba:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                else:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['labels:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                    \n",
    "                preds.append(pred)\n",
    "                \n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << initial state >>  (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << lstm_output   >>  Tensor(\"rnn/transpose_1:0\", shape=(100, 200, 128), dtype=float32)\n",
      "\n",
      "  << final state   >>  (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << logits        >>  Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
      "\n",
      "  << predictions   >>  {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n"
     ]
    }
   ],
   "source": [
    "# SentimentRNNクラスのインスタンス化\n",
    "n_words = max(list(word_to_int.values())) + 1\n",
    "\n",
    "rnn = SentimentRNN(n_words=n_words,\n",
    "                  seq_len=sequence_length,\n",
    "                  embed_size=256,\n",
    "                  lstm_size=128,\n",
    "                  num_layers=1,\n",
    "                  batch_size=100,\n",
    "                  learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40 Iteration: 20 | Train loss: 0.00027\n",
      "Epoch: 1/40 Iteration: 40 | Train loss: 0.00008\n",
      "Epoch: 1/40 Iteration: 60 | Train loss: 0.00004\n",
      "Epoch: 1/40 Iteration: 80 | Train loss: 0.00003\n",
      "Epoch: 1/40 Iteration: 100 | Train loss: 0.00003\n",
      "Epoch: 1/40 Iteration: 120 | Train loss: 0.00003\n",
      "Epoch: 1/40 Iteration: 140 | Train loss: 1.58993\n",
      "Epoch: 1/40 Iteration: 160 | Train loss: 0.03236\n",
      "Epoch: 1/40 Iteration: 180 | Train loss: 0.00912\n",
      "Epoch: 1/40 Iteration: 200 | Train loss: 0.00508\n",
      "Epoch: 1/40 Iteration: 220 | Train loss: 0.00282\n",
      "Epoch: 1/40 Iteration: 240 | Train loss: 0.00281\n",
      "Epoch: 2/40 Iteration: 260 | Train loss: 2.85194\n",
      "Epoch: 2/40 Iteration: 280 | Train loss: 0.76411\n",
      "Epoch: 2/40 Iteration: 300 | Train loss: 0.26356\n",
      "Epoch: 2/40 Iteration: 320 | Train loss: 0.12397\n",
      "Epoch: 2/40 Iteration: 340 | Train loss: 0.07835\n",
      "Epoch: 2/40 Iteration: 360 | Train loss: 0.04867\n",
      "Epoch: 2/40 Iteration: 380 | Train loss: 3.09539\n",
      "Epoch: 2/40 Iteration: 400 | Train loss: 1.13746\n",
      "Epoch: 2/40 Iteration: 420 | Train loss: 0.50909\n",
      "Epoch: 2/40 Iteration: 440 | Train loss: 0.29217\n",
      "Epoch: 2/40 Iteration: 460 | Train loss: 0.19416\n",
      "Epoch: 2/40 Iteration: 480 | Train loss: 0.13797\n",
      "Epoch: 2/40 Iteration: 500 | Train loss: 0.11372\n",
      "Epoch: 3/40 Iteration: 520 | Train loss: 1.46645\n",
      "Epoch: 3/40 Iteration: 540 | Train loss: 0.77715\n",
      "Epoch: 3/40 Iteration: 560 | Train loss: 0.46441\n",
      "Epoch: 3/40 Iteration: 580 | Train loss: 0.28415\n",
      "Epoch: 3/40 Iteration: 600 | Train loss: 0.17709\n",
      "Epoch: 3/40 Iteration: 620 | Train loss: 0.12216\n",
      "Epoch: 3/40 Iteration: 640 | Train loss: 1.48510\n",
      "Epoch: 3/40 Iteration: 660 | Train loss: 0.82802\n",
      "Epoch: 3/40 Iteration: 680 | Train loss: 0.51817\n",
      "Epoch: 3/40 Iteration: 700 | Train loss: 0.36915\n",
      "Epoch: 3/40 Iteration: 720 | Train loss: 0.25519\n",
      "Epoch: 3/40 Iteration: 740 | Train loss: 0.14900\n",
      "Epoch: 4/40 Iteration: 760 | Train loss: 1.61777\n",
      "Epoch: 4/40 Iteration: 780 | Train loss: 0.81438\n",
      "Epoch: 4/40 Iteration: 800 | Train loss: 0.27923\n",
      "Epoch: 4/40 Iteration: 820 | Train loss: 0.02772\n",
      "Epoch: 4/40 Iteration: 840 | Train loss: 0.00941\n",
      "Epoch: 4/40 Iteration: 860 | Train loss: 0.00791\n",
      "Epoch: 4/40 Iteration: 880 | Train loss: 3.65366\n",
      "Epoch: 4/40 Iteration: 900 | Train loss: 0.30285\n",
      "Epoch: 4/40 Iteration: 920 | Train loss: 0.04677\n",
      "Epoch: 4/40 Iteration: 940 | Train loss: 0.01853\n",
      "Epoch: 4/40 Iteration: 960 | Train loss: 0.00921\n",
      "Epoch: 4/40 Iteration: 980 | Train loss: 0.00842\n",
      "Epoch: 4/40 Iteration: 1000 | Train loss: 0.00625\n",
      "Epoch: 5/40 Iteration: 1020 | Train loss: 0.28292\n",
      "Epoch: 5/40 Iteration: 1040 | Train loss: 0.02980\n",
      "Epoch: 5/40 Iteration: 1060 | Train loss: 0.01514\n",
      "Epoch: 5/40 Iteration: 1080 | Train loss: 0.01148\n",
      "Epoch: 5/40 Iteration: 1100 | Train loss: 0.00785\n",
      "Epoch: 5/40 Iteration: 1120 | Train loss: 0.00695\n",
      "Epoch: 5/40 Iteration: 1140 | Train loss: 0.61667\n",
      "Epoch: 5/40 Iteration: 1160 | Train loss: 0.11147\n",
      "Epoch: 5/40 Iteration: 1180 | Train loss: 0.03674\n",
      "Epoch: 5/40 Iteration: 1200 | Train loss: 0.02107\n",
      "Epoch: 5/40 Iteration: 1220 | Train loss: 0.01424\n",
      "Epoch: 5/40 Iteration: 1240 | Train loss: 0.01269\n",
      "Epoch: 6/40 Iteration: 1260 | Train loss: 1.12697\n",
      "Epoch: 6/40 Iteration: 1280 | Train loss: 0.28776\n",
      "Epoch: 6/40 Iteration: 1300 | Train loss: 0.10559\n",
      "Epoch: 6/40 Iteration: 1320 | Train loss: 0.04958\n",
      "Epoch: 6/40 Iteration: 1340 | Train loss: 0.04264\n",
      "Epoch: 6/40 Iteration: 1360 | Train loss: 0.02830\n",
      "Epoch: 6/40 Iteration: 1380 | Train loss: 2.25807\n",
      "Epoch: 6/40 Iteration: 1400 | Train loss: 0.37993\n",
      "Epoch: 6/40 Iteration: 1420 | Train loss: 0.19859\n",
      "Epoch: 6/40 Iteration: 1440 | Train loss: 0.10556\n",
      "Epoch: 6/40 Iteration: 1460 | Train loss: 0.06049\n",
      "Epoch: 6/40 Iteration: 1480 | Train loss: 0.04285\n",
      "Epoch: 6/40 Iteration: 1500 | Train loss: 0.02838\n",
      "Epoch: 7/40 Iteration: 1520 | Train loss: 0.40782\n",
      "Epoch: 7/40 Iteration: 1540 | Train loss: 0.19912\n",
      "Epoch: 7/40 Iteration: 1560 | Train loss: 0.12786\n",
      "Epoch: 7/40 Iteration: 1580 | Train loss: 0.08194\n",
      "Epoch: 7/40 Iteration: 1600 | Train loss: 0.04717\n",
      "Epoch: 7/40 Iteration: 1620 | Train loss: 0.03031\n",
      "Epoch: 7/40 Iteration: 1640 | Train loss: 0.42411\n",
      "Epoch: 7/40 Iteration: 1660 | Train loss: 0.23021\n",
      "Epoch: 7/40 Iteration: 1680 | Train loss: 0.13462\n",
      "Epoch: 7/40 Iteration: 1700 | Train loss: 0.07992\n",
      "Epoch: 7/40 Iteration: 1720 | Train loss: 0.05221\n",
      "Epoch: 7/40 Iteration: 1740 | Train loss: 0.03938\n",
      "Epoch: 8/40 Iteration: 1760 | Train loss: 0.63730\n",
      "Epoch: 8/40 Iteration: 1780 | Train loss: 0.23617\n",
      "Epoch: 8/40 Iteration: 1800 | Train loss: 0.14868\n",
      "Epoch: 8/40 Iteration: 1820 | Train loss: 0.07000\n",
      "Epoch: 8/40 Iteration: 1840 | Train loss: 0.07315\n",
      "Epoch: 8/40 Iteration: 1860 | Train loss: 0.05750\n",
      "Epoch: 8/40 Iteration: 1880 | Train loss: 1.30311\n",
      "Epoch: 8/40 Iteration: 1900 | Train loss: 0.24430\n",
      "Epoch: 8/40 Iteration: 1920 | Train loss: 0.15526\n",
      "Epoch: 8/40 Iteration: 1940 | Train loss: 0.11708\n",
      "Epoch: 8/40 Iteration: 1960 | Train loss: 0.07726\n",
      "Epoch: 8/40 Iteration: 1980 | Train loss: 0.06153\n",
      "Epoch: 8/40 Iteration: 2000 | Train loss: 0.04363\n",
      "Epoch: 9/40 Iteration: 2020 | Train loss: 0.25793\n",
      "Epoch: 9/40 Iteration: 2040 | Train loss: 0.14889\n",
      "Epoch: 9/40 Iteration: 2060 | Train loss: 0.10482\n",
      "Epoch: 9/40 Iteration: 2080 | Train loss: 0.11905\n",
      "Epoch: 9/40 Iteration: 2100 | Train loss: 0.08114\n",
      "Epoch: 9/40 Iteration: 2120 | Train loss: 0.04570\n",
      "Epoch: 9/40 Iteration: 2140 | Train loss: 0.38000\n",
      "Epoch: 9/40 Iteration: 2160 | Train loss: 0.17444\n",
      "Epoch: 9/40 Iteration: 2180 | Train loss: 0.11647\n",
      "Epoch: 9/40 Iteration: 2200 | Train loss: 0.08665\n",
      "Epoch: 9/40 Iteration: 2220 | Train loss: 0.07192\n",
      "Epoch: 9/40 Iteration: 2240 | Train loss: 0.03993\n",
      "Epoch: 10/40 Iteration: 2260 | Train loss: 0.48679\n",
      "Epoch: 10/40 Iteration: 2280 | Train loss: 0.26606\n",
      "Epoch: 10/40 Iteration: 2300 | Train loss: 0.20538\n",
      "Epoch: 10/40 Iteration: 2320 | Train loss: 0.12011\n",
      "Epoch: 10/40 Iteration: 2340 | Train loss: 0.08836\n",
      "Epoch: 10/40 Iteration: 2360 | Train loss: 0.06434\n",
      "Epoch: 10/40 Iteration: 2380 | Train loss: 0.81406\n",
      "Epoch: 10/40 Iteration: 2400 | Train loss: 0.20851\n",
      "Epoch: 10/40 Iteration: 2420 | Train loss: 0.15292\n",
      "Epoch: 10/40 Iteration: 2440 | Train loss: 0.13513\n",
      "Epoch: 10/40 Iteration: 2460 | Train loss: 0.10761\n",
      "Epoch: 10/40 Iteration: 2480 | Train loss: 0.07413\n",
      "Epoch: 10/40 Iteration: 2500 | Train loss: 0.05898\n",
      "Epoch: 11/40 Iteration: 2520 | Train loss: 0.20199\n",
      "Epoch: 11/40 Iteration: 2540 | Train loss: 0.21232\n",
      "Epoch: 11/40 Iteration: 2560 | Train loss: 0.14770\n",
      "Epoch: 11/40 Iteration: 2580 | Train loss: 0.15765\n",
      "Epoch: 11/40 Iteration: 2600 | Train loss: 0.11388\n",
      "Epoch: 11/40 Iteration: 2620 | Train loss: 0.06074\n",
      "Epoch: 11/40 Iteration: 2640 | Train loss: 0.71565\n",
      "Epoch: 11/40 Iteration: 2660 | Train loss: 0.10813\n",
      "Epoch: 11/40 Iteration: 2680 | Train loss: 0.03964\n",
      "Epoch: 11/40 Iteration: 2700 | Train loss: 0.01371\n",
      "Epoch: 11/40 Iteration: 2720 | Train loss: 0.00865\n",
      "Epoch: 11/40 Iteration: 2740 | Train loss: 0.00694\n",
      "Epoch: 12/40 Iteration: 2760 | Train loss: 0.31315\n",
      "Epoch: 12/40 Iteration: 2780 | Train loss: 0.14357\n",
      "Epoch: 12/40 Iteration: 2800 | Train loss: 0.10391\n",
      "Epoch: 12/40 Iteration: 2820 | Train loss: 0.07964\n",
      "Epoch: 12/40 Iteration: 2840 | Train loss: 0.11631\n",
      "Epoch: 12/40 Iteration: 2860 | Train loss: 0.10490\n",
      "Epoch: 12/40 Iteration: 2880 | Train loss: 0.93170\n",
      "Epoch: 12/40 Iteration: 2900 | Train loss: 0.27542\n",
      "Epoch: 12/40 Iteration: 2920 | Train loss: 0.17180\n",
      "Epoch: 12/40 Iteration: 2940 | Train loss: 0.15699\n",
      "Epoch: 12/40 Iteration: 2960 | Train loss: 0.10687\n",
      "Epoch: 12/40 Iteration: 2980 | Train loss: 0.09581\n",
      "Epoch: 12/40 Iteration: 3000 | Train loss: 0.04136\n",
      "Epoch: 13/40 Iteration: 3020 | Train loss: 0.14417\n",
      "Epoch: 13/40 Iteration: 3040 | Train loss: 0.12879\n",
      "Epoch: 13/40 Iteration: 3060 | Train loss: 0.12317\n",
      "Epoch: 13/40 Iteration: 3080 | Train loss: 0.11256\n",
      "Epoch: 13/40 Iteration: 3100 | Train loss: 0.08742\n",
      "Epoch: 13/40 Iteration: 3120 | Train loss: 0.06573\n",
      "Epoch: 13/40 Iteration: 3140 | Train loss: 0.29838\n",
      "Epoch: 13/40 Iteration: 3160 | Train loss: 0.12812\n",
      "Epoch: 13/40 Iteration: 3180 | Train loss: 0.12994\n",
      "Epoch: 13/40 Iteration: 3200 | Train loss: 0.15382\n",
      "Epoch: 13/40 Iteration: 3220 | Train loss: 0.12994\n",
      "Epoch: 13/40 Iteration: 3240 | Train loss: 0.09535\n",
      "Epoch: 14/40 Iteration: 3260 | Train loss: 0.39798\n",
      "Epoch: 14/40 Iteration: 3280 | Train loss: 0.15355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/40 Iteration: 3300 | Train loss: 0.12962\n",
      "Epoch: 14/40 Iteration: 3320 | Train loss: 0.05517\n",
      "Epoch: 14/40 Iteration: 3340 | Train loss: 0.12987\n",
      "Epoch: 14/40 Iteration: 3360 | Train loss: 0.09286\n",
      "Epoch: 14/40 Iteration: 3380 | Train loss: 0.63470\n",
      "Epoch: 14/40 Iteration: 3400 | Train loss: 0.34697\n",
      "Epoch: 14/40 Iteration: 3420 | Train loss: 0.18320\n",
      "Epoch: 14/40 Iteration: 3440 | Train loss: 0.07953\n",
      "Epoch: 14/40 Iteration: 3460 | Train loss: 0.16688\n",
      "Epoch: 14/40 Iteration: 3480 | Train loss: 0.16607\n",
      "Epoch: 14/40 Iteration: 3500 | Train loss: 0.07785\n",
      "Epoch: 15/40 Iteration: 3520 | Train loss: 0.36752\n",
      "Epoch: 15/40 Iteration: 3540 | Train loss: 0.18010\n",
      "Epoch: 15/40 Iteration: 3560 | Train loss: 0.15982\n",
      "Epoch: 15/40 Iteration: 3580 | Train loss: 0.13659\n",
      "Epoch: 15/40 Iteration: 3600 | Train loss: 0.11236\n",
      "Epoch: 15/40 Iteration: 3620 | Train loss: 0.06498\n",
      "Epoch: 15/40 Iteration: 3640 | Train loss: 0.35236\n",
      "Epoch: 15/40 Iteration: 3660 | Train loss: 0.15630\n",
      "Epoch: 15/40 Iteration: 3680 | Train loss: 0.14147\n",
      "Epoch: 15/40 Iteration: 3700 | Train loss: 0.09063\n",
      "Epoch: 15/40 Iteration: 3720 | Train loss: 0.06300\n",
      "Epoch: 15/40 Iteration: 3740 | Train loss: 0.08762\n",
      "Epoch: 16/40 Iteration: 3760 | Train loss: 0.21511\n",
      "Epoch: 16/40 Iteration: 3780 | Train loss: 0.10863\n",
      "Epoch: 16/40 Iteration: 3800 | Train loss: 0.09950\n",
      "Epoch: 16/40 Iteration: 3820 | Train loss: 0.05753\n",
      "Epoch: 16/40 Iteration: 3840 | Train loss: 0.05849\n",
      "Epoch: 16/40 Iteration: 3860 | Train loss: 0.05006\n",
      "Epoch: 16/40 Iteration: 3880 | Train loss: 0.26120\n",
      "Epoch: 16/40 Iteration: 3900 | Train loss: 0.18008\n",
      "Epoch: 16/40 Iteration: 3920 | Train loss: 0.08703\n",
      "Epoch: 16/40 Iteration: 3940 | Train loss: 0.10426\n",
      "Epoch: 16/40 Iteration: 3960 | Train loss: 0.10510\n",
      "Epoch: 16/40 Iteration: 3980 | Train loss: 0.07833\n",
      "Epoch: 16/40 Iteration: 4000 | Train loss: 0.11153\n",
      "Epoch: 17/40 Iteration: 4020 | Train loss: 0.08550\n",
      "Epoch: 17/40 Iteration: 4040 | Train loss: 0.06422\n",
      "Epoch: 17/40 Iteration: 4060 | Train loss: 0.11392\n",
      "Epoch: 17/40 Iteration: 4080 | Train loss: 0.06419\n",
      "Epoch: 17/40 Iteration: 4100 | Train loss: 0.06280\n",
      "Epoch: 17/40 Iteration: 4120 | Train loss: 0.05731\n",
      "Epoch: 17/40 Iteration: 4140 | Train loss: 0.12306\n",
      "Epoch: 17/40 Iteration: 4160 | Train loss: 0.02894\n",
      "Epoch: 17/40 Iteration: 4180 | Train loss: 0.09948\n",
      "Epoch: 17/40 Iteration: 4200 | Train loss: 0.07537\n",
      "Epoch: 17/40 Iteration: 4220 | Train loss: 0.02519\n",
      "Epoch: 17/40 Iteration: 4240 | Train loss: 0.03454\n",
      "Epoch: 18/40 Iteration: 4260 | Train loss: 0.10073\n",
      "Epoch: 18/40 Iteration: 4280 | Train loss: 0.11242\n",
      "Epoch: 18/40 Iteration: 4300 | Train loss: 0.08018\n",
      "Epoch: 18/40 Iteration: 4320 | Train loss: 0.03191\n",
      "Epoch: 18/40 Iteration: 4340 | Train loss: 0.03694\n",
      "Epoch: 18/40 Iteration: 4360 | Train loss: 0.03200\n",
      "Epoch: 18/40 Iteration: 4380 | Train loss: 0.13357\n",
      "Epoch: 18/40 Iteration: 4400 | Train loss: 0.09842\n",
      "Epoch: 18/40 Iteration: 4420 | Train loss: 0.03678\n",
      "Epoch: 18/40 Iteration: 4440 | Train loss: 0.04702\n",
      "Epoch: 18/40 Iteration: 4460 | Train loss: 0.04832\n",
      "Epoch: 18/40 Iteration: 4480 | Train loss: 0.02702\n",
      "Epoch: 18/40 Iteration: 4500 | Train loss: 0.06496\n",
      "Epoch: 19/40 Iteration: 4520 | Train loss: 0.03999\n",
      "Epoch: 19/40 Iteration: 4540 | Train loss: 0.05804\n",
      "Epoch: 19/40 Iteration: 4560 | Train loss: 0.05486\n",
      "Epoch: 19/40 Iteration: 4580 | Train loss: 0.02382\n",
      "Epoch: 19/40 Iteration: 4600 | Train loss: 0.05522\n",
      "Epoch: 19/40 Iteration: 4620 | Train loss: 0.01656\n",
      "Epoch: 19/40 Iteration: 4640 | Train loss: 0.05108\n",
      "Epoch: 19/40 Iteration: 4660 | Train loss: 0.01226\n",
      "Epoch: 19/40 Iteration: 4680 | Train loss: 0.05892\n",
      "Epoch: 19/40 Iteration: 4700 | Train loss: 0.02795\n",
      "Epoch: 19/40 Iteration: 4720 | Train loss: 0.01820\n",
      "Epoch: 19/40 Iteration: 4740 | Train loss: 0.02578\n",
      "Epoch: 20/40 Iteration: 4760 | Train loss: 0.04022\n",
      "Epoch: 20/40 Iteration: 4780 | Train loss: 0.02806\n",
      "Epoch: 20/40 Iteration: 4800 | Train loss: 0.03993\n",
      "Epoch: 20/40 Iteration: 4820 | Train loss: 0.02343\n",
      "Epoch: 20/40 Iteration: 4840 | Train loss: 0.02680\n",
      "Epoch: 20/40 Iteration: 4860 | Train loss: 0.01795\n",
      "Epoch: 20/40 Iteration: 4880 | Train loss: 0.15690\n",
      "Epoch: 20/40 Iteration: 4900 | Train loss: 0.03966\n",
      "Epoch: 20/40 Iteration: 4920 | Train loss: 0.01695\n",
      "Epoch: 20/40 Iteration: 4940 | Train loss: 0.04818\n",
      "Epoch: 20/40 Iteration: 4960 | Train loss: 0.02587\n",
      "Epoch: 20/40 Iteration: 4980 | Train loss: 0.03313\n",
      "Epoch: 20/40 Iteration: 5000 | Train loss: 0.03484\n",
      "Epoch: 21/40 Iteration: 5020 | Train loss: 0.01079\n",
      "Epoch: 21/40 Iteration: 5040 | Train loss: 0.00966\n",
      "Epoch: 21/40 Iteration: 5060 | Train loss: 0.03878\n",
      "Epoch: 21/40 Iteration: 5080 | Train loss: 0.02499\n",
      "Epoch: 21/40 Iteration: 5100 | Train loss: 0.05078\n",
      "Epoch: 21/40 Iteration: 5120 | Train loss: 0.01047\n",
      "Epoch: 21/40 Iteration: 5140 | Train loss: 0.06139\n",
      "Epoch: 21/40 Iteration: 5160 | Train loss: 0.00381\n",
      "Epoch: 21/40 Iteration: 5180 | Train loss: 0.01360\n"
     ]
    }
   ],
   "source": [
    "rnn.train(X_train, y_train, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの推計\n",
    "preds = rnn.predict(X_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "print('Test Acc.: %.3f' % (\n",
    "      np.sum(preds == y_true) / len(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推計の実施\n",
    "proba = rnn.predict(X_test, return_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文字レベルの言語モデルとしてRNNを実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('pg2265.txt', 'r', encoding='utf-8') as f: \n",
    "    text=f.read()\n",
    "\n",
    "# 文字列の15858文字以降を取得\n",
    "text = text[15858:]\n",
    "# 集合型に変換する\n",
    "chars = set(text)\n",
    "# 文字ごとにインデックスを付与する{'t':0,}\n",
    "char2int = {ch:i for i,ch in enumerate(chars)}\n",
    "# インデックスごとに文字を当てはめる{0: 't',}\n",
    "int2char = dict(enumerate(chars))\n",
    "# char2int(インデックス)を配列にする\n",
    "text_ints = np.array([char2int[ch] for ch in text], \n",
    "                     dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(sequence, batch_size, num_steps):\n",
    "    # バッチサイズ×ステップの個数\n",
    "    tot_batch_length = batch_size * num_steps\n",
    "    # バッチの個数\n",
    "    num_batches = int(len(sequence) / tot_batch_length)\n",
    "    print(\"num_batches :: %d\" % num_batches)\n",
    "    \n",
    "    # バッチの個数×バッチサイズ×ステップの個数\n",
    "    if num_batches*tot_batch_length + 1 > len(sequence):\n",
    "        num_batches = num_batches - 1\n",
    "    \n",
    "    # x :: バッチの個数×バッチサイズ×ステップの個数\n",
    "    # y :: バッチの個数×バッチサイズ×ステップの個数を1つずらしたもの\n",
    "    x = sequence[0:num_batches*tot_batch_length]\n",
    "    y = sequence[1:num_batches*tot_batch_length+1]\n",
    "    \n",
    "    # xとyのシーケンスをバッチサイズに分割\n",
    "    x_batch_splits = np.split(x, batch_size)\n",
    "    y_batch_splits = np.split(y, batch_size)\n",
    "    \n",
    "    # それらのバッチを結合\n",
    "    # [1 1 1] + [2 2 2] -> [[1,1,1],[2,2,2]]\n",
    "    x = np.stack(x_batch_splits)\n",
    "    y = np.stack(y_batch_splits)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches :: 254\n",
      "(64, 2540)\n",
      "[57  4 63 22 57  8  3 64 63 46]\n",
      "[ 4 63 22 57  8  3 64 63 46 49]\n",
      "The Tragedie of Hamlet\n",
      "\n",
      "Actus Primus. Scoena Prima\n"
     ]
    }
   ],
   "source": [
    "# reshape_data関数のテスト\n",
    "train_x, train_y = reshape_data(text_ints, 64, 10)\n",
    "print(train_x.shape)\n",
    "print(train_x[0, :10])\n",
    "print(train_y[0, :10])\n",
    "print(''.join(int2char[i] for i in train_x[0, :50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "def create_batch_generator(data_x, data_y, num_steps):\n",
    "    batch_size, tot_batch_length = data_x.shape\n",
    "    num_batches = int(tot_batch_length/num_steps)\n",
    "    for b in range(num_batches):\n",
    "        yield(data_x[:, b*num_steps: (b+1)*num_steps], \n",
    "             data_y[:, b*num_steps: (b+1)*num_steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_char(probas, char_size, top_n=5):\n",
    "    # np.squeeze :: サイズ1の次元を削除する [[[[1][2][3]]]][[[[4][5][6]]]] -> [[1 2 3][4 5 6]]\n",
    "    p = np.squeeze(probas)\n",
    "    # np.argsort :: 並び変えたインデックスの位置を返却する\n",
    "    p[np.argsort(p)[:-top_n]] = 0.0\n",
    "    p = p / np.sum(p)\n",
    "    # np.random.choice :: ランダムに1つの値をチョイスする\n",
    "    ch_id = np.random.choice(char_size, 1, p=p)[0]\n",
    "    return ch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(object):\n",
    "    def __init__(self, num_classes, batch_size=64, \n",
    "                 num_steps=100, lstm_size=128, \n",
    "                 num_layers=1, learning_rate=0.001, \n",
    "                 keep_prob=0.5, grad_clip=5, \n",
    "                 sampling=False):\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "\n",
    "            self.build(sampling=sampling)\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "        \n",
    "    def build(self, sampling):\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1 # サンプルモード\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "            num_steps = self.num_steps   # トレーニングモード\n",
    "    \n",
    "        tf_x = tf.placeholder(tf.int32, \n",
    "                              shape=[batch_size, num_steps], \n",
    "                              name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.int32, \n",
    "                              shape=[batch_size, num_steps], \n",
    "                              name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32, \n",
    "                              name='tf_keepprob')\n",
    "\n",
    "        # One-hot encoding:\n",
    "        # depth :: 分類するクラスの数\n",
    "        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n",
    "        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n",
    "        \n",
    "        # LSTMセル(長短期記憶)を定義し、積み上げる\n",
    "        # tf.contrib.rnn.MultiRNNCell\n",
    "        cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [tf.nn.rnn_cell.DropoutWrapper(\n",
    "               tf.nn.rnn_cell.BasicLSTMCell(self.lstm_size),\n",
    "               output_keep_prob=tf_keepprob)\n",
    "             for i in range(self.num_layers)])\n",
    "        \n",
    "        # セルの初期値を設定\n",
    "        self.initial_state = cells.zero_state(\n",
    "            batch_size, tf.float32)\n",
    "        \n",
    "        # LSTMのアウトプットと最終状態を取得\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "            cells, x_onehot, \n",
    "            initial_state=self.initial_state)\n",
    "        \n",
    "        print('  << lstm_outputs  >>', lstm_outputs)\n",
    "        \n",
    "        # 2次元のテンソルに変換\n",
    "        # batch_size, num_steps, lstm_size -> batch_size*num_steps, lstm_size\n",
    "        seq_output_reshaped = tf.reshape(\n",
    "            lstm_outputs, \n",
    "            shape=[-1, self.lstm_size],\n",
    "            name='seq_output_reshaped')\n",
    "        \n",
    "        # 全結合層に渡して総入力を取得\n",
    "        logits = tf.layers.dense(\n",
    "            inputs=seq_output_reshaped, \n",
    "            units=self.num_classes,\n",
    "            activation=None,\n",
    "            name='logits')\n",
    "        \n",
    "        # ソフトマックス関数で次の文字バッチの確率を計算\n",
    "        proba = tf.nn.softmax(\n",
    "            logits, \n",
    "            name='probabilities')\n",
    "        print(proba)\n",
    "        \n",
    "        # yを2次元のテンソルに変換\n",
    "        y_reshaped = tf.reshape(\n",
    "            y_onehot, \n",
    "            shape=[-1, self.num_classes],\n",
    "            name='y_reshaped')\n",
    "        \n",
    "        # コスト関数を定義する\n",
    "        # tf.reduce_mean :: 与えたリストに入っている数値の平均値を求める関数\n",
    "        cost = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits=logits, \n",
    "                        labels=y_reshaped),\n",
    "                    name='cost')\n",
    "        \n",
    "        # 勾配発散問題を回避するための勾配刈り込み\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "                    tf.gradients(cost, tvars), \n",
    "                    self.grad_clip)\n",
    "        \n",
    "        # オプティマイザを定義\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "                    zip(grads, tvars),\n",
    "                    name='train_op')\n",
    "        \n",
    "    def train(self, train_x, train_y, \n",
    "          num_epochs, ckpt_dir='./model/'):\n",
    "\n",
    "        # チェックポイントディレクトリが存在しない場合は作成\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.mkdir(ckpt_dir)\n",
    "        \n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "\n",
    "            n_batches = int(train_x.shape[1]/self.num_steps)\n",
    "            iterations = n_batches * num_epochs\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "\n",
    "                # ネットワークをトレーニング\n",
    "                new_state = sess.run(self.initial_state)\n",
    "                loss = 0\n",
    "                \n",
    "                # ミニバッチジェネレーター\n",
    "                bgen = create_batch_generator(\n",
    "                        train_x, train_y, self.num_steps)\n",
    "                for b, (batch_x, batch_y) in enumerate(bgen, 1):\n",
    "                    iteration = epoch*n_batches + b\n",
    "                    \n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0': self.keep_prob,\n",
    "                            self.initial_state : new_state}\n",
    "                    batch_cost, _, new_state = sess.run(\n",
    "                            ['cost:0', 'train_op', \n",
    "                                self.final_state],\n",
    "                            feed_dict=feed)\n",
    "                    if iteration % 10 == 0:\n",
    "                        print('Epoch %d/%d Iteration %d'\n",
    "                              '| Training loss: %.4f' % (\n",
    "                              epoch + 1, num_epochs, \n",
    "                              iteration, batch_cost))\n",
    "\n",
    "                # トレーニング済みのモデルを保存\n",
    "                self.saver.save(\n",
    "                        sess, os.path.join(\n",
    "                            ckpt_dir, 'language_modeling.ckpt'))\n",
    "                \n",
    "    def sample(self, output_length, \n",
    "               ckpt_dir, starter_seq=\"The \"):\n",
    "        observed_seq = [ch for ch in starter_seq]        \n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess, \n",
    "                tf.train.latest_checkpoint(ckpt_dir))\n",
    "            \n",
    "            # 1: starter_seqを使ってモデルを実行\n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for ch in starter_seq:\n",
    "                x = np.zeros((1, 1))\n",
    "                x[0,0] = char2int[ch]\n",
    "                feed = {'tf_x:0': x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state], \n",
    "                        feed_dict=feed)\n",
    "\n",
    "            ch_id = get_top_char(proba, len(chars))\n",
    "            observed_seq.append(int2char[ch_id])\n",
    "            \n",
    "            # 2: 更新されたobserved_seqを使ってモデルを実行\n",
    "            for i in range(output_length):\n",
    "                x[0,0] = ch_id\n",
    "                feed = {'tf_x:0': x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state], \n",
    "                        feed_dict=feed)\n",
    "\n",
    "                ch_id = get_top_char(proba, len(chars))\n",
    "                observed_seq.append(int2char[ch_id])\n",
    "\n",
    "        return ''.join(observed_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches :: 25\n",
      "  << lstm_outputs  >> Tensor(\"rnn/transpose_1:0\", shape=(64, 100, 128), dtype=float32)\n",
      "Tensor(\"probabilities:0\", shape=(6400, 65), dtype=float32)\n",
      "Epoch 1/100 Iteration 10| Training loss: 3.6289\n",
      "Epoch 1/100 Iteration 20| Training loss: 3.3708\n",
      "Epoch 2/100 Iteration 30| Training loss: 3.2857\n",
      "Epoch 2/100 Iteration 40| Training loss: 3.2490\n",
      "Epoch 2/100 Iteration 50| Training loss: 3.2240\n",
      "Epoch 3/100 Iteration 60| Training loss: 3.2153\n",
      "Epoch 3/100 Iteration 70| Training loss: 3.1800\n",
      "Epoch 4/100 Iteration 80| Training loss: 3.1639\n",
      "Epoch 4/100 Iteration 90| Training loss: 3.1429\n",
      "Epoch 4/100 Iteration 100| Training loss: 3.1331\n",
      "Epoch 5/100 Iteration 110| Training loss: 3.1153\n",
      "Epoch 5/100 Iteration 120| Training loss: 3.0724\n",
      "Epoch 6/100 Iteration 130| Training loss: 3.0306\n",
      "Epoch 6/100 Iteration 140| Training loss: 2.9955\n",
      "Epoch 6/100 Iteration 150| Training loss: 2.9627\n",
      "Epoch 7/100 Iteration 160| Training loss: 2.9450\n",
      "Epoch 7/100 Iteration 170| Training loss: 2.8844\n",
      "Epoch 8/100 Iteration 180| Training loss: 2.8433\n",
      "Epoch 8/100 Iteration 190| Training loss: 2.8181\n",
      "Epoch 8/100 Iteration 200| Training loss: 2.7655\n",
      "Epoch 9/100 Iteration 210| Training loss: 2.7675\n",
      "Epoch 9/100 Iteration 220| Training loss: 2.7090\n",
      "Epoch 10/100 Iteration 230| Training loss: 2.6731\n",
      "Epoch 10/100 Iteration 240| Training loss: 2.6582\n",
      "Epoch 10/100 Iteration 250| Training loss: 2.6048\n",
      "Epoch 11/100 Iteration 260| Training loss: 2.6197\n",
      "Epoch 11/100 Iteration 270| Training loss: 2.5670\n",
      "Epoch 12/100 Iteration 280| Training loss: 2.5551\n",
      "Epoch 12/100 Iteration 290| Training loss: 2.5455\n",
      "Epoch 12/100 Iteration 300| Training loss: 2.4935\n",
      "Epoch 13/100 Iteration 310| Training loss: 2.5161\n",
      "Epoch 13/100 Iteration 320| Training loss: 2.4809\n",
      "Epoch 14/100 Iteration 330| Training loss: 2.4601\n",
      "Epoch 14/100 Iteration 340| Training loss: 2.4776\n",
      "Epoch 14/100 Iteration 350| Training loss: 2.4176\n",
      "Epoch 15/100 Iteration 360| Training loss: 2.4472\n",
      "Epoch 15/100 Iteration 370| Training loss: 2.4136\n",
      "Epoch 16/100 Iteration 380| Training loss: 2.4060\n",
      "Epoch 16/100 Iteration 390| Training loss: 2.4154\n",
      "Epoch 16/100 Iteration 400| Training loss: 2.3520\n",
      "Epoch 17/100 Iteration 410| Training loss: 2.4117\n",
      "Epoch 17/100 Iteration 420| Training loss: 2.3578\n",
      "Epoch 18/100 Iteration 430| Training loss: 2.3550\n",
      "Epoch 18/100 Iteration 440| Training loss: 2.3817\n",
      "Epoch 18/100 Iteration 450| Training loss: 2.3167\n",
      "Epoch 19/100 Iteration 460| Training loss: 2.3589\n",
      "Epoch 19/100 Iteration 470| Training loss: 2.3048\n",
      "Epoch 20/100 Iteration 480| Training loss: 2.3175\n",
      "Epoch 20/100 Iteration 490| Training loss: 2.3424\n",
      "Epoch 20/100 Iteration 500| Training loss: 2.2827\n",
      "Epoch 21/100 Iteration 510| Training loss: 2.3317\n",
      "Epoch 21/100 Iteration 520| Training loss: 2.2855\n",
      "Epoch 22/100 Iteration 530| Training loss: 2.2788\n",
      "Epoch 22/100 Iteration 540| Training loss: 2.3213\n",
      "Epoch 22/100 Iteration 550| Training loss: 2.2462\n",
      "Epoch 23/100 Iteration 560| Training loss: 2.3019\n",
      "Epoch 23/100 Iteration 570| Training loss: 2.2628\n",
      "Epoch 24/100 Iteration 580| Training loss: 2.2703\n",
      "Epoch 24/100 Iteration 590| Training loss: 2.2948\n",
      "Epoch 24/100 Iteration 600| Training loss: 2.2177\n",
      "Epoch 25/100 Iteration 610| Training loss: 2.2881\n",
      "Epoch 25/100 Iteration 620| Training loss: 2.2442\n",
      "Epoch 26/100 Iteration 630| Training loss: 2.2343\n",
      "Epoch 26/100 Iteration 640| Training loss: 2.2644\n",
      "Epoch 26/100 Iteration 650| Training loss: 2.2030\n",
      "Epoch 27/100 Iteration 660| Training loss: 2.2487\n",
      "Epoch 27/100 Iteration 670| Training loss: 2.2149\n",
      "Epoch 28/100 Iteration 680| Training loss: 2.2179\n",
      "Epoch 28/100 Iteration 690| Training loss: 2.2468\n",
      "Epoch 28/100 Iteration 700| Training loss: 2.1776\n",
      "Epoch 29/100 Iteration 710| Training loss: 2.2411\n",
      "Epoch 29/100 Iteration 720| Training loss: 2.1888\n",
      "Epoch 30/100 Iteration 730| Training loss: 2.1977\n",
      "Epoch 30/100 Iteration 740| Training loss: 2.2311\n",
      "Epoch 30/100 Iteration 750| Training loss: 2.1632\n",
      "Epoch 31/100 Iteration 760| Training loss: 2.2159\n",
      "Epoch 31/100 Iteration 770| Training loss: 2.1796\n",
      "Epoch 32/100 Iteration 780| Training loss: 2.1849\n",
      "Epoch 32/100 Iteration 790| Training loss: 2.2038\n",
      "Epoch 32/100 Iteration 800| Training loss: 2.1349\n",
      "Epoch 33/100 Iteration 810| Training loss: 2.1948\n",
      "Epoch 33/100 Iteration 820| Training loss: 2.1670\n",
      "Epoch 34/100 Iteration 830| Training loss: 2.1559\n",
      "Epoch 34/100 Iteration 840| Training loss: 2.1836\n",
      "Epoch 34/100 Iteration 850| Training loss: 2.1214\n",
      "Epoch 35/100 Iteration 860| Training loss: 2.1778\n",
      "Epoch 35/100 Iteration 870| Training loss: 2.1508\n",
      "Epoch 36/100 Iteration 880| Training loss: 2.1515\n",
      "Epoch 36/100 Iteration 890| Training loss: 2.1684\n",
      "Epoch 36/100 Iteration 900| Training loss: 2.1084\n",
      "Epoch 37/100 Iteration 910| Training loss: 2.1576\n",
      "Epoch 37/100 Iteration 920| Training loss: 2.1293\n",
      "Epoch 38/100 Iteration 930| Training loss: 2.1414\n",
      "Epoch 38/100 Iteration 940| Training loss: 2.1640\n",
      "Epoch 38/100 Iteration 950| Training loss: 2.1096\n",
      "Epoch 39/100 Iteration 960| Training loss: 2.1520\n",
      "Epoch 39/100 Iteration 970| Training loss: 2.1087\n",
      "Epoch 40/100 Iteration 980| Training loss: 2.1177\n",
      "Epoch 40/100 Iteration 990| Training loss: 2.1500\n",
      "Epoch 40/100 Iteration 1000| Training loss: 2.0748\n",
      "Epoch 41/100 Iteration 1010| Training loss: 2.1311\n",
      "Epoch 41/100 Iteration 1020| Training loss: 2.1077\n",
      "Epoch 42/100 Iteration 1030| Training loss: 2.1081\n",
      "Epoch 42/100 Iteration 1040| Training loss: 2.1310\n",
      "Epoch 42/100 Iteration 1050| Training loss: 2.0599\n",
      "Epoch 43/100 Iteration 1060| Training loss: 2.1280\n",
      "Epoch 43/100 Iteration 1070| Training loss: 2.0960\n",
      "Epoch 44/100 Iteration 1080| Training loss: 2.0979\n",
      "Epoch 44/100 Iteration 1090| Training loss: 2.1260\n",
      "Epoch 44/100 Iteration 1100| Training loss: 2.0484\n",
      "Epoch 45/100 Iteration 1110| Training loss: 2.1068\n",
      "Epoch 45/100 Iteration 1120| Training loss: 2.0733\n",
      "Epoch 46/100 Iteration 1130| Training loss: 2.0929\n",
      "Epoch 46/100 Iteration 1140| Training loss: 2.1097\n",
      "Epoch 46/100 Iteration 1150| Training loss: 2.0461\n",
      "Epoch 47/100 Iteration 1160| Training loss: 2.0944\n",
      "Epoch 47/100 Iteration 1170| Training loss: 2.0622\n",
      "Epoch 48/100 Iteration 1180| Training loss: 2.0653\n",
      "Epoch 48/100 Iteration 1190| Training loss: 2.0878\n",
      "Epoch 48/100 Iteration 1200| Training loss: 2.0316\n",
      "Epoch 49/100 Iteration 1210| Training loss: 2.0690\n",
      "Epoch 49/100 Iteration 1220| Training loss: 2.0503\n",
      "Epoch 50/100 Iteration 1230| Training loss: 2.0551\n",
      "Epoch 50/100 Iteration 1240| Training loss: 2.0710\n",
      "Epoch 50/100 Iteration 1250| Training loss: 2.0187\n",
      "Epoch 51/100 Iteration 1260| Training loss: 2.0827\n",
      "Epoch 51/100 Iteration 1270| Training loss: 2.0401\n",
      "Epoch 52/100 Iteration 1280| Training loss: 2.0541\n",
      "Epoch 52/100 Iteration 1290| Training loss: 2.0787\n",
      "Epoch 52/100 Iteration 1300| Training loss: 1.9963\n",
      "Epoch 53/100 Iteration 1310| Training loss: 2.0679\n",
      "Epoch 53/100 Iteration 1320| Training loss: 2.0277\n",
      "Epoch 54/100 Iteration 1330| Training loss: 2.0347\n",
      "Epoch 54/100 Iteration 1340| Training loss: 2.0616\n",
      "Epoch 54/100 Iteration 1350| Training loss: 1.9960\n",
      "Epoch 55/100 Iteration 1360| Training loss: 2.0406\n",
      "Epoch 55/100 Iteration 1370| Training loss: 2.0280\n",
      "Epoch 56/100 Iteration 1380| Training loss: 2.0241\n",
      "Epoch 56/100 Iteration 1390| Training loss: 2.0520\n",
      "Epoch 56/100 Iteration 1400| Training loss: 1.9765\n",
      "Epoch 57/100 Iteration 1410| Training loss: 2.0501\n",
      "Epoch 57/100 Iteration 1420| Training loss: 2.0182\n",
      "Epoch 58/100 Iteration 1430| Training loss: 2.0094\n",
      "Epoch 58/100 Iteration 1440| Training loss: 2.0454\n",
      "Epoch 58/100 Iteration 1450| Training loss: 1.9878\n",
      "Epoch 59/100 Iteration 1460| Training loss: 2.0277\n",
      "Epoch 59/100 Iteration 1470| Training loss: 2.0076\n",
      "Epoch 60/100 Iteration 1480| Training loss: 2.0191\n",
      "Epoch 60/100 Iteration 1490| Training loss: 2.0307\n",
      "Epoch 60/100 Iteration 1500| Training loss: 1.9663\n",
      "Epoch 61/100 Iteration 1510| Training loss: 2.0202\n",
      "Epoch 61/100 Iteration 1520| Training loss: 1.9970\n",
      "Epoch 62/100 Iteration 1530| Training loss: 2.0072\n",
      "Epoch 62/100 Iteration 1540| Training loss: 2.0229\n",
      "Epoch 62/100 Iteration 1550| Training loss: 1.9635\n",
      "Epoch 63/100 Iteration 1560| Training loss: 2.0182\n",
      "Epoch 63/100 Iteration 1570| Training loss: 1.9822\n",
      "Epoch 64/100 Iteration 1580| Training loss: 1.9950\n",
      "Epoch 64/100 Iteration 1590| Training loss: 2.0197\n",
      "Epoch 64/100 Iteration 1600| Training loss: 1.9477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100 Iteration 1610| Training loss: 1.9990\n",
      "Epoch 65/100 Iteration 1620| Training loss: 1.9786\n",
      "Epoch 66/100 Iteration 1630| Training loss: 1.9748\n",
      "Epoch 66/100 Iteration 1640| Training loss: 2.0000\n",
      "Epoch 66/100 Iteration 1650| Training loss: 1.9396\n",
      "Epoch 67/100 Iteration 1660| Training loss: 1.9993\n",
      "Epoch 67/100 Iteration 1670| Training loss: 1.9641\n",
      "Epoch 68/100 Iteration 1680| Training loss: 1.9769\n",
      "Epoch 68/100 Iteration 1690| Training loss: 2.0020\n",
      "Epoch 68/100 Iteration 1700| Training loss: 1.9398\n",
      "Epoch 69/100 Iteration 1710| Training loss: 1.9912\n",
      "Epoch 69/100 Iteration 1720| Training loss: 1.9642\n",
      "Epoch 70/100 Iteration 1730| Training loss: 1.9720\n",
      "Epoch 70/100 Iteration 1740| Training loss: 1.9885\n",
      "Epoch 70/100 Iteration 1750| Training loss: 1.9215\n",
      "Epoch 71/100 Iteration 1760| Training loss: 1.9840\n",
      "Epoch 71/100 Iteration 1770| Training loss: 1.9495\n",
      "Epoch 72/100 Iteration 1780| Training loss: 1.9705\n",
      "Epoch 72/100 Iteration 1790| Training loss: 1.9832\n",
      "Epoch 72/100 Iteration 1800| Training loss: 1.9175\n",
      "Epoch 73/100 Iteration 1810| Training loss: 1.9707\n",
      "Epoch 73/100 Iteration 1820| Training loss: 1.9371\n",
      "Epoch 74/100 Iteration 1830| Training loss: 1.9542\n",
      "Epoch 74/100 Iteration 1840| Training loss: 1.9842\n",
      "Epoch 74/100 Iteration 1850| Training loss: 1.9140\n",
      "Epoch 75/100 Iteration 1860| Training loss: 1.9663\n",
      "Epoch 75/100 Iteration 1870| Training loss: 1.9420\n",
      "Epoch 76/100 Iteration 1880| Training loss: 1.9405\n",
      "Epoch 76/100 Iteration 1890| Training loss: 1.9616\n",
      "Epoch 76/100 Iteration 1900| Training loss: 1.9103\n",
      "Epoch 77/100 Iteration 1910| Training loss: 1.9645\n",
      "Epoch 77/100 Iteration 1920| Training loss: 1.9331\n",
      "Epoch 78/100 Iteration 1930| Training loss: 1.9361\n",
      "Epoch 78/100 Iteration 1940| Training loss: 1.9493\n",
      "Epoch 78/100 Iteration 1950| Training loss: 1.8991\n",
      "Epoch 79/100 Iteration 1960| Training loss: 1.9491\n",
      "Epoch 79/100 Iteration 1970| Training loss: 1.9298\n",
      "Epoch 80/100 Iteration 1980| Training loss: 1.9384\n",
      "Epoch 80/100 Iteration 1990| Training loss: 1.9452\n",
      "Epoch 80/100 Iteration 2000| Training loss: 1.8966\n",
      "Epoch 81/100 Iteration 2010| Training loss: 1.9425\n",
      "Epoch 81/100 Iteration 2020| Training loss: 1.9272\n",
      "Epoch 82/100 Iteration 2030| Training loss: 1.9211\n",
      "Epoch 82/100 Iteration 2040| Training loss: 1.9487\n",
      "Epoch 82/100 Iteration 2050| Training loss: 1.8884\n",
      "Epoch 83/100 Iteration 2060| Training loss: 1.9405\n",
      "Epoch 83/100 Iteration 2070| Training loss: 1.8976\n",
      "Epoch 84/100 Iteration 2080| Training loss: 1.9051\n",
      "Epoch 84/100 Iteration 2090| Training loss: 1.9379\n",
      "Epoch 84/100 Iteration 2100| Training loss: 1.8846\n",
      "Epoch 85/100 Iteration 2110| Training loss: 1.9284\n",
      "Epoch 85/100 Iteration 2120| Training loss: 1.9036\n",
      "Epoch 86/100 Iteration 2130| Training loss: 1.9098\n",
      "Epoch 86/100 Iteration 2140| Training loss: 1.9263\n",
      "Epoch 86/100 Iteration 2150| Training loss: 1.8768\n",
      "Epoch 87/100 Iteration 2160| Training loss: 1.9274\n",
      "Epoch 87/100 Iteration 2170| Training loss: 1.8959\n",
      "Epoch 88/100 Iteration 2180| Training loss: 1.9026\n",
      "Epoch 88/100 Iteration 2190| Training loss: 1.9307\n",
      "Epoch 88/100 Iteration 2200| Training loss: 1.8739\n",
      "Epoch 89/100 Iteration 2210| Training loss: 1.9232\n",
      "Epoch 89/100 Iteration 2220| Training loss: 1.8876\n",
      "Epoch 90/100 Iteration 2230| Training loss: 1.8954\n",
      "Epoch 90/100 Iteration 2240| Training loss: 1.9266\n",
      "Epoch 90/100 Iteration 2250| Training loss: 1.8645\n",
      "Epoch 91/100 Iteration 2260| Training loss: 1.9156\n",
      "Epoch 91/100 Iteration 2270| Training loss: 1.8904\n",
      "Epoch 92/100 Iteration 2280| Training loss: 1.8935\n",
      "Epoch 92/100 Iteration 2290| Training loss: 1.9068\n",
      "Epoch 92/100 Iteration 2300| Training loss: 1.8540\n",
      "Epoch 93/100 Iteration 2310| Training loss: 1.9154\n",
      "Epoch 93/100 Iteration 2320| Training loss: 1.8820\n",
      "Epoch 94/100 Iteration 2330| Training loss: 1.8833\n",
      "Epoch 94/100 Iteration 2340| Training loss: 1.9186\n",
      "Epoch 94/100 Iteration 2350| Training loss: 1.8609\n",
      "Epoch 95/100 Iteration 2360| Training loss: 1.9048\n",
      "Epoch 95/100 Iteration 2370| Training loss: 1.8797\n",
      "Epoch 96/100 Iteration 2380| Training loss: 1.8888\n",
      "Epoch 96/100 Iteration 2390| Training loss: 1.9066\n",
      "Epoch 96/100 Iteration 2400| Training loss: 1.8444\n",
      "Epoch 97/100 Iteration 2410| Training loss: 1.8904\n",
      "Epoch 97/100 Iteration 2420| Training loss: 1.8726\n",
      "Epoch 98/100 Iteration 2430| Training loss: 1.8841\n",
      "Epoch 98/100 Iteration 2440| Training loss: 1.8978\n",
      "Epoch 98/100 Iteration 2450| Training loss: 1.8377\n",
      "Epoch 99/100 Iteration 2460| Training loss: 1.8792\n",
      "Epoch 99/100 Iteration 2470| Training loss: 1.8693\n",
      "Epoch 100/100 Iteration 2480| Training loss: 1.8627\n",
      "Epoch 100/100 Iteration 2490| Training loss: 1.8919\n",
      "Epoch 100/100 Iteration 2500| Training loss: 1.8440\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_steps = 100 \n",
    "train_x, train_y = reshape_data(text_ints, \n",
    "                                batch_size, \n",
    "                                num_steps)\n",
    "\n",
    "rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)\n",
    "rnn.train(train_x, train_y, \n",
    "          num_epochs=100,\n",
    "          ckpt_dir='./model-100/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
